import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object firstexample {
def main(args: Array[String])
{
val conf = new SparkConf().setAppName("Raja")
val sc = new SparkContext(conf)
val dataRDD = sc.textFile("file:///root/spark-prac/data.txt")
dataRDD.saveAsTextFile("file:///root/spark-prac/dataop")
}
}



from pyspark import SparkConf,SparkContext
conf = SparkConf().setAppName("PyhtonAPP")
sc = SparkContext(conf = conf)
file = sc.textFile("file:///root/spark-prac/data.txt")
for line in file.collect():
 print(line)

spark-submit --master yarn-cluster --executor-memory 1G --num-executors 3 2nd.py




import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.storage.StorageLevel._

object firstexample {
def main(args: Array[String])
{
val conf = new SparkConf().setAppName("Raja")
val sc = new SparkContext(conf)
val dataRDD = sc.textFile("file:///root/spark-prac/data.txt")
dataRDD.persist(MEMORY_AND_DISK)
dataRDD.saveAsTextFile("file:///root/spark-prac/dataop")
}



import org.apache.spark.SparkConf
import.org.apche.spark.SparkContext
import org.apache.spark.SQLContext

val conf = new Sparkconf().setAppName("Ex")
val sc = new SparkContext(conf)
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

case class orders(order_id:String,order_date:String,cust_id:String,status:String)

val ordersRDD = sc.textFile("file:///root/spark-prac/data/orders.data")
val latestRDD = ordersRDD.map(x => x.split(",")).map(x => orders(x(0),x(1),x(2),x(3)))

import sqlContext.implicits._
val dfex = latestRDD.toDF()
dfex.registerTempTable("dfex")

val querydata = sqlContext.sql("select order_id,status from dfex")
querydata.collect().foreach(println)

}


import org.apche.spark.SparkConf
import org.apache.spark.SparkContext

object latest {
def main(args: Array[String])
{

val conf = new SparkConf().set()
val sc = new SparkContext(conf)
val deptRDD = sc.textFile("file:///root/spark-prac/data/dept.data")
val catRDD = sc.textFile("file:///root/spark-prac/data/categories.data")

val mapRDD = deptRDD.map(x => x.split(",")).map(x => (x(0).toInt,x(1)))
val catmapRDD = catRDD.map(x => x.split(",")).map(x => (x(1).toInt,x(2)))

val joinRDD = mapRDD.join(catmapRDD)

joinRDD.saveAsTextFile("file:///root/spark-prac/data/joinopbrack")

}


