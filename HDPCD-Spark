import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object firstexample {
def main(args: Array[String])
{
val conf = new SparkConf().setAppName("Raja")
val sc = new SparkContext(conf)
val dataRDD = sc.textFile("file:///root/spark-prac/data.txt")
dataRDD.saveAsTextFile("file:///root/spark-prac/dataop")
}
}



from pyspark import SparkConf,SparkContext
conf = SparkConf().setAppName("PyhtonAPP")
sc = SparkContext(conf = conf)
file = sc.textFile("file:///root/spark-prac/data.txt")
for line in file.collect():
 print(line)

spark-submit --master yarn-cluster --executor-memory 1G --num-executors 3 2nd.py




import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.storage.StorageLevel._

object firstexample {
def main(args: Array[String])
{
val conf = new SparkConf().setAppName("Raja")
val sc = new SparkContext(conf)
val dataRDD = sc.textFile("file:///root/spark-prac/data.txt")
dataRDD.persist(MEMORY_AND_DISK)
dataRDD.saveAsTextFile("file:///root/spark-prac/dataop")
}



import org.apache.spark.SparkConf
import.org.apche.spark.SparkContext
import org.apache.spark.SQLContext

val conf = new Sparkconf().setAppName("Ex")
val sc = new SparkContext(conf)
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

case class orders(order_id:String,order_date:String,cust_id:String,status:String)

val ordersRDD = sc.textFile("file:///root/spark-prac/data/orders.data")
val latestRDD = ordersRDD.map(x => x.split(",")).map(x => orders(x(0),x(1),x(2),x(3)))

import sqlContext.implicits._
val dfex = latestRDD.toDF()
dfex.registerTempTable("dfex")

val querydata = sqlContext.sql("select order_id,status from dfex")
querydata.collect().foreach(println)

}


import org.apche.spark.SparkConf
import org.apache.spark.SparkContext

object latest {
def main(args: Array[String])
{

val conf = new SparkConf().set()
val sc = new SparkContext(conf)
val deptRDD = sc.textFile("file:///root/spark-prac/data/dept.data")
val catRDD = sc.textFile("file:///root/spark-prac/data/categories.data")

val mapRDD = deptRDD.map(x => x.split(",")).map(x => (x(0).toInt,x(1)))
val catmapRDD = catRDD.map(x => x.split(",")).map(x => (x(1).toInt,x(2)))

val joinRDD = mapRDD.join(catmapRDD)

joinRDD.saveAsTextFile("file:///root/spark-prac/data/joinopbrack")

}


import org.apache.spark.sql.hive.orc._
import org.apache.spark.sql._

val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

hiveContext.sql("create table yahoo_orc_table (date STRING,  volume INT, adj_price FLOAT) stored as orc")
val yahoo_stocks = sc.textFile("hdfs://sandbox.hortonworks.com:8020/tmp/yahoo_stocks.csv")

#Seperating Header
val header = yahoo_stocks.first
#Seperate Data
val data = yahoo_stocks.mapPartitionsWithIndex { (idx, iter) => if (idx == 0) iter.drop(1) else iter }

case class YahooStockPrice(date: String, open: Float, high: Float, low: Float, close: Float, volume: Integer, adjClose: Float)

val stockprice = data.map(_.split(",")).map(row => YahooStockPrice(row(0), row(1).trim.toFloat, row(2).trim.toFloat,  row(6).trim.toFloat)).toDF()
stockprice.registerTempTable("yahoo_stocks_temp")
val results = sqlContext.sql("SELECT * FROM yahoo_stocks_temp")

results.map(t => "Stock Entry: " + t.toString).collect().foreach(println)
results.write.format("orc").save("yahoo_stocks_orc")


#Reading ORC
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
val yahoo_stocks_orc = hiveContext.read.format("orc").load("yahoo_stocks_orc")

yahoo_stocks_orc.registerTempTable("orcTest")
hiveContext.sql("SELECT * from orcTest").collect.foreach(println)

#Writing as a CSV file 
df.map(x => x.mkString("|")).saveAsTextFile("file.csv")

