import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object firstexample {
def main(args: Array[String])
{
val conf = new SparkConf().setAppName("Raja")
val sc = new SparkContext(conf)
val dataRDD = sc.textFile("file:///root/spark-prac/data.txt")
dataRDD.saveAsTextFile("file:///root/spark-prac/dataop")
}
}



from pyspark import SparkConf,SparkContext
conf = SparkConf().setAppName("PyhtonAPP")
sc = SparkContext(conf = conf)
file = sc.textFile("file:///root/spark-prac/data.txt")
for line in file.collect():
 print(line)

spark-submit --master yarn-cluster --executor-memory 1G --num-executors 3 2nd.py




import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import org.apache.spark.storage.StorageLevel._

object firstexample {
def main(args: Array[String])
{
val conf = new SparkConf().setAppName("Raja")
val sc = new SparkContext(conf)
val dataRDD = sc.textFile("file:///root/spark-prac/data.txt")
dataRDD.persist(MEMORY_AND_DISK)
dataRDD.saveAsTextFile("file:///root/spark-prac/dataop")
}
}


import org.apche.spark.SparkConf
import org.apache.spark.SparkContext

object latest {
def main(args: Array[String])
{

val conf = new SparkConf().set()
val sc = new SparkContext(conf)
val deptRDD = sc.textFile("file:///root/spark-prac/data/dept.data")
val catRDD = sc.textFile("file:///root/spark-prac/data/categories.data")

val mapRDD = deptRDD.map(x => x.split(",")).map(x => (x(0).toInt,x(1)))
val catmapRDD = catRDD.map(x => x.split(",")).map(x => (x(1).toInt,x(2)))

val joinRDD = mapRDD.join(catmapRDD)

joinRDD.saveAsTextFile("file:///root/spark-prac/data/joinopbrack")

}


